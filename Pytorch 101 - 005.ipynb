{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pytorch 101 - 005\n",
    "#### *Date* : 2019.05.26\n",
    "#### *Auther* :`Jen-Huan Hu`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Let's review ```nn.Conv2d``` first "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch device using cuda\n",
      "We have a callable convolution Conv2d(3, 2, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "with weights Parameter containing:\n",
      "tensor([[[[ 0.1170,  0.1327,  0.1763],\n",
      "          [-0.1010,  0.1747, -0.0708],\n",
      "          [ 0.1855, -0.1766,  0.0171]],\n",
      "\n",
      "         [[-0.0423,  0.1756,  0.1107],\n",
      "          [-0.1809,  0.1849,  0.0903],\n",
      "          [-0.0586, -0.0817, -0.1330]],\n",
      "\n",
      "         [[-0.1005,  0.1882, -0.1384],\n",
      "          [ 0.1120, -0.0500, -0.1065],\n",
      "          [ 0.1580, -0.0640, -0.0662]]],\n",
      "\n",
      "\n",
      "        [[[ 0.0221,  0.1694,  0.1233],\n",
      "          [ 0.0340, -0.0098, -0.0784],\n",
      "          [ 0.1606, -0.0991,  0.0396]],\n",
      "\n",
      "         [[ 0.0662,  0.1246, -0.0520],\n",
      "          [-0.0314, -0.0763, -0.1480],\n",
      "          [-0.0309, -0.0221,  0.0753]],\n",
      "\n",
      "         [[-0.1048, -0.0192,  0.1869],\n",
      "          [ 0.0012,  0.0352, -0.0426],\n",
      "          [-0.1420, -0.0227, -0.0885]]]], requires_grad=True) and bias Parameter containing:\n",
      "tensor([0.0780, 0.0432], requires_grad=True)\n",
      "Estimated output size is (5, 4)\n",
      "Conv(a) = tensor([[[[-0.0390,  4.2346,  0.6110,  1.6433],\n",
      "          [ 5.9576,  1.2600, -5.0124,  3.6550],\n",
      "          [10.0859, -5.6555, -1.1906, -5.7556],\n",
      "          [-3.3734,  5.8092, -0.1221,  0.4681],\n",
      "          [ 2.0949, -4.3697, 15.6823, -1.9402]],\n",
      "\n",
      "         [[-0.3159,  3.5110,  9.3239,  2.8936],\n",
      "          [ 4.6777,  0.7106, -0.3316, -5.1171],\n",
      "          [-4.9483, -2.7524, -5.9016, -1.0388],\n",
      "          [ 0.7104, 10.2308,  4.7397,  0.4887],\n",
      "          [ 0.9390,  2.8202,  0.3018, -0.4625]]]],\n",
      "       grad_fn=<ThnnConv2DBackward>)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "device = 'cuda' if torch.cuda.device_count() else 'cpu'\n",
    "print('torch device using {}'.format( device ))\n",
    "\n",
    "def add_batch_dim(x):\n",
    "    # Assume x has shape of order : C, H, W !\n",
    "    x = x[:,:,:, None]\n",
    "    x = x.permute(3, 0, 1, 2)\n",
    "    return x\n",
    "\n",
    "C, H, W = 3, 5, 4\n",
    "a = torch.randn((C, H, W)) * 10\n",
    "a = add_batch_dim(a)\n",
    "Conv1 = torch.nn.Conv2d(3, 2, 3, padding = 1) # Provide callable functor\n",
    "print(\"We have a callable convolution %s\" % Conv1)\n",
    "print(\"with weights %s and bias %s\" % (Conv1.weight, Conv1.bias) )\n",
    "# DBG : print(Conv1.kernel_size, Conv1.stride, Conv1.padding)\n",
    "\n",
    "def output_size_formula(x, conv):\n",
    "    def the_formula(x_i, padding_i, k_i, stride_i ):\n",
    "        # DBG : print(x_i, padding_i, k_i, stride_i)\n",
    "        import math\n",
    "        # Formula is sth like : floor( ( input size + 2 * padding - kernelsize ) / stride ) + 1\n",
    "        return math.floor( ( x_i + 2 * padding_i - k_i ) / stride_i ) + 1\n",
    "    outsize = []\n",
    "    x_shape = x.shape[-2:] # N, C, H, W\n",
    "    # DBG : print(x_shape)\n",
    "    for i in range(len(conv.kernel_size)):\n",
    "        outsize += [ the_formula(x_shape[i], conv.padding[i], conv.kernel_size[i], conv.stride[i]) ]\n",
    "    return tuple(outsize)\n",
    "\n",
    "print(\"Estimated output size is {}\".format( output_size_formula( a, Conv1 ) ) )\n",
    "\n",
    "print(\"Conv(a) = %s\" % Conv1(a))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### And then take a glimpse of ```nn.MaxPool2d```\n",
    "Ex. \n",
    "* m = nn.MaxPool2d(3, stride=2)  \n",
    "* m = nn.MaxPool2d((3, 2), stride=(2, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[[-0.0390,  4.2346,  1.6433],\n",
      "          [10.0859,  1.2600,  3.6550],\n",
      "          [ 2.0949, 15.6823,  0.4681]],\n",
      "\n",
      "         [[-0.3159,  9.3239,  2.8936],\n",
      "          [ 4.6777,  0.7106, -1.0388],\n",
      "          [ 0.9390, 10.2308,  0.4887]]]],\n",
      "       grad_fn=<MaxPool2DWithIndicesBackward>)\n"
     ]
    }
   ],
   "source": [
    "b = Conv1( a )\n",
    "Max1 = torch.nn.MaxPool2d(2, 2, padding = 1)\n",
    "print( Max1(b))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### And also ```nn.BatchNorm2d```\n",
    "\n",
    "* num_features – CC from an expected input of size (N, C, H, W)(N,C,H,W)\n",
    "\n",
    "* eps – a value added to the denominator for numerical stability. Default: 1e-5\n",
    "\n",
    "* momentum – the value used for the running_mean and running_var computation. Can be set to None for cumulative moving average (i.e. simple average). Default: 0.1\n",
    "\n",
    "* affine – a boolean value that when set to True, this module has learnable affine parameters. Default: True\n",
    "\n",
    "* track_running_stats – a boolean value that when set to True, this module tracks the running mean and variance, and when set to False, this module does not track such statistics and always uses batch statistics in both training and eval modes. Default: True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[[-0.7463, -0.0184, -0.4598],\n",
      "          [ 0.9782, -0.5251, -0.1171],\n",
      "          [-0.3829,  1.9314, -0.6600]],\n",
      "\n",
      "         [[-0.0590,  0.1074, -0.0036],\n",
      "          [ 0.0272, -0.0413, -0.0715],\n",
      "          [-0.0373,  0.1231, -0.0451]]]], grad_fn=<NativeBatchNormBackward>)\n"
     ]
    }
   ],
   "source": [
    "channels = b.shape[1] # N, C, H, W\n",
    "BN1 = torch.nn.BatchNorm2d( channels )\n",
    "c = Max1(b)\n",
    "print(BN1(c))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Today we are going to get familiar with \n",
    "1. ```torch.nn.Module```\n",
    "2. ```torch.nn.Sequential```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class Model(nn.Module):   \n",
    "    def __init__(self):   \n",
    "        super(Model, self).__init__()   \n",
    "        self.conv1 = nn.Conv2d(1, 20, 5)   \n",
    "        self.conv2 = nn.Conv2d(20, 20, 5)   \n",
    "\n",
    "    def forward(self, x):     \n",
    "       x = F.relu(self.conv1(x))    \n",
    "       return F.relu(self.conv2(x))    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DemoModel(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(DemoModel, self).__init__()\n",
    "        self.conv1 = torch.nn.Conv2d(3, 4, 3, padding = 1)\n",
    "        self.max1 = torch.nn.MaxPool2d(2, 2, padding = 1)\n",
    "        self.BN1 = torch.nn.BatchNorm2d( 4 )\n",
    "        self.conv2 = torch.nn.Conv2d(4, 8, 3, padding = 1)\n",
    "        self.max2 = torch.nn.MaxPool2d(2, 2, padding = 1)\n",
    "        self.BN2 = torch.nn.BatchNorm2d( 8 )\n",
    "    def forward(self, x):\n",
    "        x = self.BN1( self.max1( torch.relu( self.conv1( x ) ) ) )\n",
    "        x = self.BN2( self.max2( torch.relu( self.conv2( x ) ) ) )\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### And now I want to know if the DemoModel is constructed as I expected\n",
    "> I will print the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DemoModel(\n",
      "  (conv1): Conv2d(3, 4, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (max1): MaxPool2d(kernel_size=2, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
      "  (BN1): BatchNorm2d(4, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (conv2): Conv2d(4, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (max2): MaxPool2d(kernel_size=2, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
      "  (BN2): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "A = DemoModel()\n",
    "print(A)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Now let's do it with ```torch.nn.Sequential```\n",
    "And borrow some code from internets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Flatten(torch.nn.Module):\n",
    "    def forward(self, x):\n",
    "        return x.view(x.size(0), -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import OrderedDict\n",
    "class DemoModel2(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(DemoModel2, self).__init__()\n",
    "        self.conv1 = torch.nn.Conv2d(3, 4, 3, padding = 1)\n",
    "        self.max1 = torch.nn.MaxPool2d(3, 3, padding = 1)\n",
    "        self.BN1 = torch.nn.BatchNorm2d( 4 )\n",
    "        self.conv2 = torch.nn.Conv2d(4, 8, 3, padding = 1)\n",
    "        self.max2 = torch.nn.MaxPool2d(3, 3, padding = 1)\n",
    "        self.BN2 = torch.nn.BatchNorm2d( 8 )\n",
    "        self.flat2 = Flatten()\n",
    "        self.fc2 = torch.nn.Linear(32, 5, True)\n",
    "        \n",
    "        self.combined = torch.nn.Sequential( OrderedDict([\n",
    "              ('conv1', self.conv1),\n",
    "              ('relu1', torch.nn.ReLU()),\n",
    "              ('max1', self.max1),\n",
    "              ('BN1', self.BN1),\n",
    "              ('conv2', self.conv2),\n",
    "              ('relu2', torch.nn.ReLU()),\n",
    "              ('max2', self.max2),\n",
    "              ('BN2', self.BN2),\n",
    "              ('flat2', self.flat2),\n",
    "              ('fc2', self.fc2)\n",
    "        ]) )\n",
    "    def forward(self, x):\n",
    "        return self.combined(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DemoModel2(\n",
      "  (conv1): Conv2d(3, 4, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (max1): MaxPool2d(kernel_size=3, stride=3, padding=1, dilation=1, ceil_mode=False)\n",
      "  (BN1): BatchNorm2d(4, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (conv2): Conv2d(4, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (max2): MaxPool2d(kernel_size=3, stride=3, padding=1, dilation=1, ceil_mode=False)\n",
      "  (BN2): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (flat2): Flatten()\n",
      "  (fc2): Linear(in_features=32, out_features=5, bias=True)\n",
      "  (combined): Sequential(\n",
      "    (conv1): Conv2d(3, 4, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (relu1): ReLU()\n",
      "    (max1): MaxPool2d(kernel_size=3, stride=3, padding=1, dilation=1, ceil_mode=False)\n",
      "    (BN1): BatchNorm2d(4, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (conv2): Conv2d(4, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (relu2): ReLU()\n",
      "    (max2): MaxPool2d(kernel_size=3, stride=3, padding=1, dilation=1, ceil_mode=False)\n",
      "    (BN2): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (flat2): Flatten()\n",
      "    (fc2): Linear(in_features=32, out_features=5, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "B = DemoModel2()\n",
    "print(B)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([5, 3, 10, 10])\n",
      "torch.Size([5, 8, 4, 4])\n",
      "torch.Size([5, 5])\n",
      "tensor([[-0.2187, -0.2665, -0.1491,  0.0903, -0.2394],\n",
      "        [ 0.1285, -0.0998,  0.1162,  0.1700, -0.2390],\n",
      "        [-0.3168, -0.0243,  0.0458, -0.2027, -0.0322],\n",
      "        [-0.1828, -0.4088,  0.0997, -0.3462, -0.1795],\n",
      "        [-0.1259, -0.4868, -0.0021, -0.2592, -0.2578]],\n",
      "       grad_fn=<AddmmBackward>)\n"
     ]
    }
   ],
   "source": [
    "x = torch.randn((5, 3, 10, 10))\n",
    "print(x.shape)\n",
    "\n",
    "a1 = A(x)\n",
    "b1 = B(x)\n",
    "print(a1.shape)\n",
    "print(b1.shape)\n",
    "print(b1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
