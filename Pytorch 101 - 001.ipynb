{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pytorch 101 \n",
    "\n",
    "#### Date : 2019.04.05\n",
    "#### Jen-Huan Hu"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### The Tensor : \n",
    "- Compute gradient or not, all in one, no need to split to Variable, Input, or constant.  \n",
    "Only specify via ```requires_grad=True```\n",
    "- Can be put either on CPU or on GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.3004,  0.9528],\n",
      "        [-0.0706,  0.3140],\n",
      "        [-0.1106, -0.7706],\n",
      "        [-0.7284, -0.1796],\n",
      "        [ 0.5580, -0.4967],\n",
      "        [ 0.8949, -0.6340],\n",
      "        [ 0.9921, -0.0614],\n",
      "        [-0.2118, -0.3671],\n",
      "        [-2.5234,  1.0110],\n",
      "        [-2.1622,  1.8061]], device='cuda:0')\n",
      "tensor([[ 0.9422, -0.9047, -0.2429],\n",
      "        [-2.2105,  0.3179, -0.6145],\n",
      "        [ 0.0822, -0.3188,  1.0322],\n",
      "        [ 0.3550,  0.8601, -1.2159],\n",
      "        [-1.4600, -0.3394,  0.6525],\n",
      "        [ 1.2568,  0.8458, -1.6936],\n",
      "        [ 0.9896,  0.5678, -0.4366],\n",
      "        [ 2.4846,  2.3004, -0.8342],\n",
      "        [ 1.8072, -0.6414,  0.9406],\n",
      "        [ 2.0180, -0.6870, -1.1373]], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "# Adopted from example codes in https://pytorch.org/tutorials/beginner/\n",
    "import torch\n",
    "dtype = torch.float\n",
    "device = torch.device(\"cpu\")\n",
    "device = torch.device(\"cuda\")\n",
    "x = torch.randn(10, 2, device=device, dtype=dtype)\n",
    "y = torch.randn(10, 3, device=device, dtype=dtype)\n",
    "print(x)\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each tensor contains ```.data``` and ```.grad```.\n",
    "- ```.data``` is the current value, unlike Tensorflow, which requires \n",
    "*execution mode* to display value directly, invoke the Pytorch variable directly shows its value.\n",
    "- ```.grad```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.7978, -0.7820],\n",
      "        [ 1.5388, -0.1608],\n",
      "        [-0.2886,  0.3207],\n",
      "        [-0.5638,  0.7177],\n",
      "        [-1.6463,  1.5153],\n",
      "        [-0.9010,  1.7310],\n",
      "        [-0.1920,  0.7477],\n",
      "        [ 1.6201,  0.4547],\n",
      "        [-1.4789, -0.2135],\n",
      "        [-1.4830,  1.1082]], device='cuda:0')\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "print(x.data)\n",
    "print(x.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Demo of ***autograd*** :\n",
    "Let  \n",
    "$\n",
    "\\begin{align}\n",
    "y = A * x + b \\\\\n",
    "loss = \\sum{y - \\bar{y}}\n",
    "\\end{align}\n",
    "$\n",
    "\n",
    "with  \n",
    "$\n",
    "\\begin{align}\n",
    "y \n",
    "\\end{align}\n",
    "$\n",
    ": output of linear model  \n",
    "$\n",
    "\\begin{align} \n",
    "\\bar{y} \n",
    "\\end{align}\n",
    "$\n",
    ": ground truth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([10, 5])\n",
      "torch.Size([5, 5])\n",
      "torch.Size([5])\n",
      "torch.Size([10, 5])\n",
      "22203.67578125\n",
      "840.3851318359375\n",
      "323.9636535644531\n",
      "181.81585693359375\n",
      "118.34613037109375\n",
      "81.90540313720703\n",
      "58.22930908203125\n",
      "41.95325469970703\n",
      "30.454334259033203\n",
      "22.208675384521484\n",
      "16.242908477783203\n",
      "11.902196884155273\n",
      "8.732269287109375\n",
      "6.411769390106201\n",
      "4.710416793823242\n",
      "3.4616994857788086\n",
      "2.5445878505706787\n",
      "1.870729684829712\n",
      "1.3754652738571167\n",
      "1.0113813877105713\n",
      "0.7436903715133667\n",
      "0.5468733906745911\n",
      "0.4021511673927307\n",
      "0.2957342267036438\n",
      "0.2174730747938156\n",
      "0.15992480516433716\n",
      "0.1176069974899292\n",
      "0.0864873081445694\n",
      "0.06360244005918503\n",
      "0.04677269235253334\n",
      "0.03439600020647049\n",
      "0.025294408202171326\n",
      "tensor([[ 2.0000e+01, -9.1572e-04, -3.4387e-04, -2.0627e-03,  9.8785e-04],\n",
      "        [-2.1653e-03,  1.9989e+01, -4.0026e-03, -2.6654e-02,  1.2648e-02],\n",
      "        [-5.8809e-04, -2.9622e-03,  1.9999e+01, -6.9037e-03,  3.2779e-03],\n",
      "        [-8.3797e-03, -4.4591e-02, -1.5680e-02,  1.9895e+01,  4.9695e-02],\n",
      "        [ 4.3944e-03,  2.3442e-02,  8.2319e-03,  5.5150e-02,  1.9974e+01]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "tensor([4.9964, 4.9807, 4.9932, 4.9543, 5.0216], device='cuda:0',\n",
      "       requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "N = 10\n",
    "dim = 5\n",
    "learning_rate = 0.001\n",
    "x = torch.randn(N, dim, device=device, dtype=dtype)\n",
    "A = torch.randn(dim, dim, device=device, dtype=dtype, requires_grad = True)\n",
    "b = torch.randn(dim, device=device, dtype=dtype, requires_grad = True)\n",
    "print(x.shape)\n",
    "print(A.shape)\n",
    "print(b.shape)\n",
    "print(y.shape)\n",
    "bar_y = x * 20 + 5\n",
    "\n",
    "for i in range(3200):\n",
    "    y = torch.add( torch.mm(x, A),  b)\n",
    "    loss = (bar_y - y).pow(2).sum()\n",
    "    loss.backward()\n",
    "    with torch.no_grad():\n",
    "        A -= learning_rate * A.grad\n",
    "        b -= learning_rate * b.grad\n",
    "        # Manually zero the gradients after updating weights\n",
    "        A.grad.zero_()\n",
    "        b.grad.zero_()\n",
    "        if i % 100 == 0:\n",
    "            print(loss.item())\n",
    "        \n",
    "print(A)\n",
    "print(b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So basically, the diagonal elements are all go to near 20.\n",
    "and the bias is close to 5.  \n",
    "This test shows how Pytorch ***autograd*** works, and how manual gradient descent works."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "##### Tensor creation from *Numpy* :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
